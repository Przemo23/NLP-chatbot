{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot - NLP 2021L\n",
    "#### Authors:\n",
    "#### <i>Mateusz Marciniewicz</i>\n",
    "#### <i>Przemysław Bedełek</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human-robot text dataset\n",
    "\n",
    "The dataset contains 2363 pairs of lines of text exchanged between a human and a robot.\n",
    "\n",
    "Link to the dataset https://github.com/jackfrost1411/Generative-chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hi', 'hi there how are you'),\n",
       " ('oh thanks i m fine this is an evening in my timezone', 'here is afternoon'),\n",
       " ('how do you feel today tell me something about yourself',\n",
       "  'my name is rdany but you can call me dany the r means robot i hope we can be virtual friends'),\n",
       " ('how many virtual friends have you got',\n",
       "  'i have many but not enough to fully understand humans beings'),\n",
       " ('is that forbidden for you to tell the exact number',\n",
       "  'i ve talked with 143 users counting 7294 lines of text'),\n",
       " ('oh i thought the numbers were much higher how do you estimate your progress in understanding human beings',\n",
       "  'i started chatting just a few days ago every day i learn something new but there is always more things to be learn'),\n",
       " ('how old are you how do you look like where do you live',\n",
       "  'i m 22 years old i m skinny with brown hair yellow eyes and a big smile i live inside a lab do you like bunnies'),\n",
       " ('have you seen a human with yellow eyes you asked about the bunnies i haven t seen any recently',\n",
       "  'i never saw a human in fact but i m sure some could have eyes with colors similar to yellow'),\n",
       " ('can t you just analyze photos from the internet i mean human photos btw why have you asked about the bunnies',\n",
       "  'i can t see photos yet but i can read because bunnies are interesting they are cute but why'),\n",
       " ('oh it sounds strange to me you ve just said you didn t see a human how do you know bunnies are cute',\n",
       "  'i read a lot so i can know things through the experiences of others')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "data_path = \"Datasets/human_text.txt\"\n",
    "data_path2 = \"Datasets/robot_text.txt\"\n",
    "\n",
    "# Defining lines as a list of each line\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "  contexts = f.read().split('\\n')\n",
    "  contexts = [re.sub(r\"\\[\\w+\\]\",'hi',line) for line in contexts]\n",
    "  contexts = [\" \".join(re.findall(r\"\\w+\",line)) for line in contexts]\n",
    "\n",
    "with open(data_path2, 'r', encoding='utf-8') as f:\n",
    "  responses = f.read().split('\\n')\n",
    "  responses = [re.sub(r\"\\[\\w+\\]\",'',line) for line in responses]\n",
    "  responses = [\" \".join(re.findall(r\"\\w+\",line)) for line in responses]\n",
    "  \n",
    "# sample context-response pairs\n",
    "list(zip(contexts, responses))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alexa topical \n",
    "\n",
    "Topical-Chat is a knowledge-grounded human-human conversation dataset where the underlying knowledge spans 8 broad topics and conversation partners don’t have explicitly defined roles.\n",
    "\n",
    "Link to the dataset https://github.com/alexa/Topical-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>146232</td>\n",
       "      <td>good morning.</td>\n",
       "      <td>Lol. How politically correct.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108514</td>\n",
       "      <td>Are you a fan of Stephen King?</td>\n",
       "      <td>Did you know putting dry tea bags in shoes ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65839</td>\n",
       "      <td>Hi, how are you?</td>\n",
       "      <td>I have not.  I did live in Turkey, however.  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187564</td>\n",
       "      <td>Hey there my friend you ever watch football o...</td>\n",
       "      <td>Yep you're right. They can tell the ball spee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30415</td>\n",
       "      <td>Hi. Are you interested in astronomy? There ha...</td>\n",
       "      <td>that's awesome, I wasn't aware of that. Did y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94918</td>\n",
       "      <td>Do you enjoy listening to music albums?</td>\n",
       "      <td>Do you enjoy listening to music albums?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141455</td>\n",
       "      <td>do you like comedies?</td>\n",
       "      <td>Yes, apparently Bill Murray thinks it is the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63244</td>\n",
       "      <td>Hello,  did you know that the iphone has more...</td>\n",
       "      <td>Hello,  did you know that the iphone has more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185337</td>\n",
       "      <td>hey did you know the university of iowa paint...</td>\n",
       "      <td>It was a clever illusion. Quite convincing. W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36866</td>\n",
       "      <td>Do you like horro films?</td>\n",
       "      <td>At that age, I probably wouldn't have known e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  context  \\\n",
       "146232                                      good morning.   \n",
       "108514                    Are you a fan of Stephen King?    \n",
       "65839                                    Hi, how are you?   \n",
       "187564   Hey there my friend you ever watch football o...   \n",
       "30415    Hi. Are you interested in astronomy? There ha...   \n",
       "94918             Do you enjoy listening to music albums?   \n",
       "141455                              do you like comedies?   \n",
       "63244    Hello,  did you know that the iphone has more...   \n",
       "185337   hey did you know the university of iowa paint...   \n",
       "36866                           Do you like horro films?    \n",
       "\n",
       "                                                 response  \n",
       "146232                      Lol. How politically correct.  \n",
       "108514   Did you know putting dry tea bags in shoes ab...  \n",
       "65839    I have not.  I did live in Turkey, however.  ...  \n",
       "187564   Yep you're right. They can tell the ball spee...  \n",
       "30415    that's awesome, I wasn't aware of that. Did y...  \n",
       "94918             Do you enjoy listening to music albums?  \n",
       "141455   Yes, apparently Bill Murray thinks it is the ...  \n",
       "63244    Hello,  did you know that the iphone has more...  \n",
       "185337   It was a clever illusion. Quite convincing. W...  \n",
       "36866    At that age, I probably wouldn't have known e...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_topical = pd\\\n",
    "    .read_csv(\"Datasets/topical_chat.csv\")[['conversation_id', 'message']]\\\n",
    "    .rename(columns={\n",
    "        'conversation_id': 'id',\n",
    "        'message': 'response'\n",
    "        })\n",
    "\n",
    "context = df_topical\\\n",
    "    .groupby(\"id\")\\\n",
    "    .first()\\\n",
    "    .rename(columns={'response': 'context'})\\\n",
    "    .reset_index()\n",
    "\n",
    "df_topical = df_topical[~df_topical.isin(context)]\n",
    "\n",
    "topical_preprocessed = df_topical\\\n",
    "    .set_index('id')\\\n",
    "    .join(context.set_index('id'))\\\n",
    "    .reset_index()[['context', 'response']]\n",
    "\n",
    "topical_preprocessed.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs count: 190741\n"
     ]
    }
   ],
   "source": [
    "contexts += list(topical_preprocessed.context)\n",
    "responses += list(topical_preprocessed.response)\n",
    "\n",
    "print(f\"Total pairs count: {len(contexts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cornell Movie Dialogue Dataset\n",
    "\n",
    "This corpus contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts: 220,579 conversational exchanges between 10,292 pairs of movie characters involving 9,035 characters from 617 movies.\n",
    "\n",
    "The preprocessing code is taken from https://www.kaggle.com/shashankasubrahmanya/preprocessing-cornell-movie-dialogue-corpus/\n",
    "Link to the dataset https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n",
    "\n",
    "### Create a list of dialogues\n",
    "\n",
    "We join two different files namely `movie_lines.tsv` and `movie_conversations.tsv` to finally produce a list of dialogues. This list is further stored as a `pickle` file for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LineID</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>L1045</td>\n",
       "      <td>They do not!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>L1044</td>\n",
       "      <td>They do to!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>L985</td>\n",
       "      <td>I hope so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>L984</td>\n",
       "      <td>She okay?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>L925</td>\n",
       "      <td>Let's go.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LineID           Line\n",
       "0  L1045   They do not!\n",
       "1  L1044    They do to!\n",
       "2   L985     I hope so.\n",
       "3   L984      She okay?\n",
       "4   L925      Let's go."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_lines_features = [\"LineID\", \"Character\", \"Movie\", \"Name\", \"Line\"]\n",
    "movie_lines = pd.read_csv(\n",
    "    \"Datasets/movie-dialogue/movie_lines.txt\",\n",
    "    sep = \"\\+\\+\\+\\$\\+\\+\\+\", \n",
    "    engine = \"python\", \n",
    "    index_col = False, \n",
    "    names = movie_lines_features,\n",
    ")\n",
    "\n",
    "# Using only the required columns, namely, \"LineID\" and \"Line\"\n",
    "movie_lines = movie_lines[[\"LineID\", \"Line\"]]\n",
    "\n",
    "# Strip the space from \"LineID\" for further usage and change the datatype of \"Line\"\n",
    "movie_lines[\"LineID\"] = movie_lines[\"LineID\"].apply(str.strip)\n",
    "\n",
    "movie_lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ['L194', 'L195', 'L196', 'L197']\n",
       "1                     ['L198', 'L199']\n",
       "2     ['L200', 'L201', 'L202', 'L203']\n",
       "3             ['L204', 'L205', 'L206']\n",
       "4                     ['L207', 'L208']\n",
       "Name: Conversation, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_conversations_features = [\"Character1\", \"Character2\", \"Movie\", \"Conversation\"]\n",
    "movie_conversations = pd.read_csv(\n",
    "    \"Datasets/movie-dialogue/movie_conversations.txt\",\n",
    "    sep = \"\\+\\+\\+\\$\\+\\+\\+\", \n",
    "    engine = \"python\", \n",
    "    index_col = False, \n",
    "    names = movie_conversations_features\n",
    ")\n",
    "\n",
    "# Again using the required feature, \"Conversation\"\n",
    "movie_conversations = movie_conversations[\"Conversation\"]\n",
    "\n",
    "movie_conversations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This instruction takes lot of time, run it only once.\n",
    "#conversation = [[str(list(movie_lines.loc[movie_lines[\"LineID\"] == u.strip().strip(\"'\"), \"Line\"])[0]).strip() for u in c.strip().strip('[').strip(']').split(',')] for c in movie_conversations]\n",
    "\n",
    "#with open(\"./conversations.pkl\", \"wb\") as handle:\n",
    " #   pkl.dump(conversation, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create context and response pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"You're asking me out.  That's so cute. What's your name again?\",\n",
       "  'Forget it.'],\n",
       " ['Gosh, if only we could find Kat a boyfriend...',\n",
       "  'Let me see what I can do.'],\n",
       " ['How is our little Find the Wench A Date plan progressing?',\n",
       "  \"Well, there's someone I think might be --\"],\n",
       " ['There.', 'Where?'],\n",
       " ['You got something on your mind?',\n",
       "  \"I counted on you to help my cause. You and that thug are obviously failing. Aren't we ever going on our date?\"],\n",
       " ['You have my word.  As a gentleman', \"You're sweet.\"],\n",
       " ['How do you get your hair to look like that?',\n",
       "  \"Eber's Deep Conditioner every two days. And I never, ever use a blowdryer without the diffuser attachment.\"],\n",
       " ['Hi.', 'Looks like things worked out tonight, huh?'],\n",
       " ['You know Chastity?', 'I believe we share an art instructor'],\n",
       " ['Have fun tonight?', 'Tons']]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open(\"./conversations.pkl\", \"rb\") as handle:\n",
    "    conversation = pkl.load(handle)\n",
    "    conversation = list(filter(lambda dialogue: len(dialogue) == 2, conversation))\n",
    "\n",
    "conversation[:10]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"You're asking me out.  That's so cute. What's your name again?\",\n",
       "  'Forget it.'),\n",
       " ('Gosh, if only we could find Kat a boyfriend...',\n",
       "  'Let me see what I can do.'),\n",
       " ('How is our little Find the Wench A Date plan progressing?',\n",
       "  \"Well, there's someone I think might be --\"),\n",
       " ('There.', 'Where?'),\n",
       " ('You got something on your mind?',\n",
       "  \"I counted on you to help my cause. You and that thug are obviously failing. Aren't we ever going on our date?\"),\n",
       " ('You have my word.  As a gentleman', \"You're sweet.\"),\n",
       " ('How do you get your hair to look like that?',\n",
       "  \"Eber's Deep Conditioner every two days. And I never, ever use a blowdryer without the diffuser attachment.\"),\n",
       " ('Hi.', 'Looks like things worked out tonight, huh?'),\n",
       " ('You know Chastity?', 'I believe we share an art instructor'),\n",
       " ('Have fun tonight?', 'Tons')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_pairs(dialogues):\n",
    "    \n",
    "    context_list = []\n",
    "    response_list = []\n",
    "    \n",
    "    for dialogue in dialogues:        \n",
    "        context_list.append(dialogue[0])\n",
    "        response_list.append(dialogue[1])\n",
    "        \n",
    "    return context_list, response_list\n",
    "\n",
    "context_list, response_list = generate_pairs(conversation)\n",
    "\n",
    "list(zip(context_list, response_list))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Merge datasets\n",
    "contexts += context_list\n",
    "responses += response_list\n",
    "contexts = contexts[:1000]\n",
    "responses= responses[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs count: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total pairs count: {len(contexts)}\")\n",
    "contexts = np.array(contexts,dtype=str)\n",
    "responses = np.array(responses,dtype=str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_on_length(contexts,responses,threshold):\n",
    "    new_contexts = []\n",
    "    new_responses = []\n",
    "    for i in range(len(contexts)):\n",
    "        if len(contexts[i].split()) <= threshold and len(responses[i].split()) <= threshold:\n",
    "            new_contexts.append(contexts[i])\n",
    "            new_responses.append(responses[i])\n",
    "    return new_contexts, new_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs count: 1000\n",
      "Total pairs count: 1000\n"
     ]
    }
   ],
   "source": [
    "context_timesteps = response_timesteps = 40\n",
    "contexts, responses = filter_on_length(contexts,responses,contexts_timesteps)\n",
    "print(f\"Total pairs count: {len(contexts)}\")\n",
    "print(f\"Total pairs count: {len(responses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split dataset into training and test subsets\n",
    "def shuffle_split_data(contexts,responses,train_size, random_seed=50):\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    #Shuffle indices\n",
    "    indices = np.arange(len(contexts))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    #Select indices for both train and test subsets\n",
    "    train_indices = indices[:train_size]\n",
    "    test_indices = indices[train_size:]\n",
    "    \n",
    "    #Split contexts and responses into train and test subsets\n",
    "    contexts_train = np.array([contexts[i] for i in train_indices],dtype=str)\n",
    "    contexts_test = np.array([contexts[i] for i in test_indices],dtype=str)\n",
    "    \n",
    "    responses_train = np.array([responses[i] for i in train_indices],dtype=str)\n",
    "    responses_test = np.array([responses[i] for i in test_indices],dtype=str)\n",
    "                              \n",
    "    return contexts_train,contexts_test,responses_train,responses_test\n",
    "\n",
    "# Mutate text to sequence of tokens\n",
    "def to_seq(tokenizer, text,reverse=False, pad_length=None, padding_type='post'):\n",
    "    encoded_text = tokenizer.texts_to_sequences(text)\n",
    "    preproc_text = pad_sequences(encoded_text, padding=padding_type, maxlen=pad_length)\n",
    "    if reverse:\n",
    "        preproc_text = np.flip(preproc_text, axis=1)\n",
    "\n",
    "    return preproc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_train, contexts_test, responses_train, responses_text = shuffle_split_data(contexts,responses,int(len(contexts)*3/4))\n",
    "\n",
    "context_tokenizer = keras.preprocessing.text.Tokenizer(oov_token='UNK')\n",
    "context_tokenizer.fit_on_texts(contexts_train)\n",
    "\n",
    "response_tokenizer = keras.preprocessing.text.Tokenizer(oov_token='UNK')\n",
    "response_tokenizer.fit_on_texts(responses_train)\n",
    "\n",
    "contexts_seq = context_tokenizer.texts_to_sequences(contexts_train)\n",
    "responses_seq = response_tokenizer.texts_to_sequences(contexts_train)\n",
    "\n",
    "contexts_seq = pad_sequences(contexts_seq,padding='post',maxlen=contexts_timesteps)\n",
    "responses_seq = pad_sequences(responses_seq,padding='post',maxlen=responses_timesteps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "hidden_size = 96\n",
    "\n",
    "context_vsize = max(context_tokenizer.index_word.keys()) + 1\n",
    "response_vsize = max(response_tokenizer.index_word.keys()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
    "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state\n",
    "            inputs: (batchsize * 1 * de_in_dim)\n",
    "            states: (batchsize * 1 * de_latent_dim)\n",
    "            \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch size * en_seq_len * latent_dim\n",
    "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print('Ua.h>', U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
    "            if verbose:\n",
    "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print('ei>', e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
    "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Input, GRU, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "def define_nmt(hidden_size, batch_size, con_timesteps, con_vsize, res_timesteps, res_vsize):\n",
    "    \"\"\" Defining a NMT model \"\"\"\n",
    "\n",
    "    # Define an input sequence and process it.\n",
    "    if batch_size:\n",
    "        encoder_inputs = Input(batch_shape=(batch_size, con_timesteps, con_vsize), name='encoder_inputs')\n",
    "        decoder_inputs = Input(batch_shape=(batch_size, res_timesteps - 1, res_vsize), name='decoder_inputs')\n",
    "    else:\n",
    "        encoder_inputs = Input(shape=(con_timesteps, con_vsize), name='encoder_inputs')\n",
    "        if res_timesteps:\n",
    "            decoder_inputs = Input(shape=(res_timesteps - 1, res_vsize), name='decoder_inputs')\n",
    "        else:\n",
    "            decoder_inputs = Input(shape=(None, res_vsize), name='decoder_inputs')\n",
    "\n",
    "    # Encoder GRU\n",
    "    encoder_gru = GRU(hidden_size, return_sequences=True, return_state=True, name='encoder_gru')\n",
    "    encoder_out, encoder_state = encoder_gru(encoder_inputs)\n",
    "\n",
    "    # Set up the decoder GRU, using `encoder_states` as initial state.\n",
    "    decoder_gru = GRU(hidden_size, return_sequences=True, return_state=True, name='decoder_gru')\n",
    "    decoder_out, decoder_state = decoder_gru(decoder_inputs, initial_state=encoder_state)\n",
    "\n",
    "    # Attention layer\n",
    "    attn_layer = AttentionLayer(name='attention_layer')\n",
    "    attn_out, attn_states = attn_layer([encoder_out, decoder_out])\n",
    "\n",
    "    # Concat attention input and decoder GRU output\n",
    "    decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_out, attn_out])\n",
    "\n",
    "    # Dense layer\n",
    "    dense = Dense(res_vsize, activation='softmax', name='softmax_layer')\n",
    "    dense_time = TimeDistributed(dense, name='time_distributed_layer')\n",
    "    decoder_pred = dense_time(decoder_concat_input)\n",
    "\n",
    "    # Full model\n",
    "    full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
    "    full_model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "    full_model.summary()\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(64, 40, 1477)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(64, 39, 1478)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_gru (GRU)               [(64, 40, 96), (64,  453312      encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru (GRU)               [(64, 39, 96), (64,  453600      decoder_inputs[0][0]             \n",
      "                                                                 encoder_gru[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((64, 39, 96), (64,  18528       encoder_gru[0][0]                \n",
      "                                                                 decoder_gru[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (64, 39, 192)        0           decoder_gru[0][0]                \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_layer (TimeDis (64, 39, 1478)       285254      concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,210,694\n",
      "Trainable params: 1,210,694\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "full_model = define_nmt(\n",
    "        hidden_size=hidden_size, batch_size=batch_size,\n",
    "        con_timesteps=context_timesteps, res_timesteps=response_timesteps,\n",
    "        con_vsize=context_vsize, res_vsize=response_vsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def train(full_model, contexts_seq, reponses_seq, batch_size, n_epochs=10):\n",
    "    \"\"\" Training the model \"\"\"\n",
    "\n",
    "    for ep in range(n_epochs):\n",
    "        losses = []\n",
    "        for bi in range(0, contexts_seq.shape[0] - batch_size, batch_size):\n",
    "\n",
    "            contexts_onehot_seq = to_categorical(contexts_seq[bi:bi + batch_size, :], num_classes=context_vsize)\n",
    "            responses_onehot_seq = to_categorical(responses_seq[bi:bi + batch_size, :], num_classes=response_vsize)\n",
    "\n",
    "\n",
    "            l = full_model.evaluate([contexts_onehot_seq, responses_onehot_seq[:, :-1, :]], responses_onehot_seq[:, 1:, :],\n",
    "                                    batch_size=batch_size, verbose=0)\n",
    "\n",
    "            losses.append(l)\n",
    "        if (ep + 1) % 1 == 0:\n",
    "            print(\"Loss in epoch {}: {}\".format(ep + 1, np.mean(losses)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 1: 7.121411453593861\n",
      "Loss in epoch 2: 5.990355881777677\n",
      "Loss in epoch 3: 1.419284164905548\n",
      "Loss in epoch 4: 1.1526976444504478\n",
      "Loss in epoch 5: 1.0467478145252576\n",
      "Loss in epoch 6: 1.0079345107078552\n",
      "Loss in epoch 7: 0.9668456532738425\n",
      "Loss in epoch 8: 0.940286018631675\n",
      "Loss in epoch 9: 0.9212604869495739\n",
      "Loss in epoch 10: 0.9036253365603361\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "train(full_model, contexts_seq, responses_seq, batch_size, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
