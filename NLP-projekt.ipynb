{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot - NLP 2021L\n",
    "#### Authors:\n",
    "#### <i>Mateusz Marciniewicz</i>\n",
    "#### <i>Przemysław Bedełek</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human-robot text dataset\n",
    "\n",
    "The dataset contains 2363 pairs of lines of text exchanged between a human and a robot.\n",
    "\n",
    "Link to the dataset https://github.com/jackfrost1411/Generative-chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hi', 'hi there how are you'),\n",
       " ('oh thanks i m fine this is an evening in my timezone', 'here is afternoon'),\n",
       " ('how do you feel today tell me something about yourself',\n",
       "  'my name is rdany but you can call me dany the r means robot i hope we can be virtual friends'),\n",
       " ('how many virtual friends have you got',\n",
       "  'i have many but not enough to fully understand humans beings'),\n",
       " ('is that forbidden for you to tell the exact number',\n",
       "  'i ve talked with 143 users counting 7294 lines of text'),\n",
       " ('oh i thought the numbers were much higher how do you estimate your progress in understanding human beings',\n",
       "  'i started chatting just a few days ago every day i learn something new but there is always more things to be learn'),\n",
       " ('how old are you how do you look like where do you live',\n",
       "  'i m 22 years old i m skinny with brown hair yellow eyes and a big smile i live inside a lab do you like bunnies'),\n",
       " ('have you seen a human with yellow eyes you asked about the bunnies i haven t seen any recently',\n",
       "  'i never saw a human in fact but i m sure some could have eyes with colors similar to yellow'),\n",
       " ('can t you just analyze photos from the internet i mean human photos btw why have you asked about the bunnies',\n",
       "  'i can t see photos yet but i can read because bunnies are interesting they are cute but why'),\n",
       " ('oh it sounds strange to me you ve just said you didn t see a human how do you know bunnies are cute',\n",
       "  'i read a lot so i can know things through the experiences of others')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "data_path = \"Datasets/human_text.txt\"\n",
    "data_path2 = \"Datasets/robot_text.txt\"\n",
    "\n",
    "# Defining lines as a list of each line\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "  contexts = f.read().split('\\n')\n",
    "  contexts = [re.sub(r\"\\[\\w+\\]\",'hi',line) for line in contexts]\n",
    "  contexts = [\" \".join(re.findall(r\"\\w+\",line)) for line in contexts]\n",
    "\n",
    "with open(data_path2, 'r', encoding='utf-8') as f:\n",
    "  responses = f.read().split('\\n')\n",
    "  responses = [re.sub(r\"\\[\\w+\\]\",'',line) for line in responses]\n",
    "  responses = [\" \".join(re.findall(r\"\\w+\",line)) for line in responses]\n",
    "  \n",
    "# sample context-response pairs\n",
    "list(zip(contexts, responses))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alexa topical \n",
    "\n",
    "Topical-Chat is a knowledge-grounded human-human conversation dataset where the underlying knowledge spans 8 broad topics and conversation partners don’t have explicitly defined roles.\n",
    "\n",
    "Link to the dataset https://github.com/alexa/Topical-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>126284</td>\n",
       "      <td>Hi do you know above details of comedy film</td>\n",
       "      <td>yes,i like comedy u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75277</td>\n",
       "      <td>Hello there, are you a Johnny Depp fan?</td>\n",
       "      <td>I am too. Do you use Netflix?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30709</td>\n",
       "      <td>I feel so bad for Queen Elizabeth. Her corgi ...</td>\n",
       "      <td>Yes they were. I spend way too much time on m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66143</td>\n",
       "      <td>I believe that service dogs are such a necess...</td>\n",
       "      <td>I do too but it's difficult not to see the mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102820</td>\n",
       "      <td>Good morning! Did you know that Kim Jong Un h...</td>\n",
       "      <td>The u.s. president's guest house is larger th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95585</td>\n",
       "      <td>Hey, how are you?</td>\n",
       "      <td>Huh... Sure, it would be a great invention in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168151</td>\n",
       "      <td>Are you a NFL fan?</td>\n",
       "      <td>I don't have a favorite team per se. I often ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14155</td>\n",
       "      <td>Hello, how are you doing tonight?</td>\n",
       "      <td>Yeah.  Do you play golf? Babe Ruth was once t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29707</td>\n",
       "      <td>Hi, how are you?</td>\n",
       "      <td>I love linkin park. Do you listen to them?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171399</td>\n",
       "      <td>Hello,  do you like football?</td>\n",
       "      <td>Do you follow the other sports more than foot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  context  \\\n",
       "126284        Hi do you know above details of comedy film   \n",
       "75277             Hello there, are you a Johnny Depp fan?   \n",
       "30709    I feel so bad for Queen Elizabeth. Her corgi ...   \n",
       "66143    I believe that service dogs are such a necess...   \n",
       "102820   Good morning! Did you know that Kim Jong Un h...   \n",
       "95585                                   Hey, how are you?   \n",
       "168151                                 Are you a NFL fan?   \n",
       "14155                   Hello, how are you doing tonight?   \n",
       "29707                                    Hi, how are you?   \n",
       "171399                      Hello,  do you like football?   \n",
       "\n",
       "                                                 response  \n",
       "126284                                yes,i like comedy u  \n",
       "75277                       I am too. Do you use Netflix?  \n",
       "30709    Yes they were. I spend way too much time on m...  \n",
       "66143    I do too but it's difficult not to see the mo...  \n",
       "102820   The u.s. president's guest house is larger th...  \n",
       "95585    Huh... Sure, it would be a great invention in...  \n",
       "168151   I don't have a favorite team per se. I often ...  \n",
       "14155    Yeah.  Do you play golf? Babe Ruth was once t...  \n",
       "29707          I love linkin park. Do you listen to them?  \n",
       "171399   Do you follow the other sports more than foot...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_topical = pd\\\n",
    "    .read_csv(\"Datasets/topical_chat.csv\")[['conversation_id', 'message']]\\\n",
    "    .rename(columns={\n",
    "        'conversation_id': 'id',\n",
    "        'message': 'response'\n",
    "        })\n",
    "\n",
    "context = df_topical\\\n",
    "    .groupby(\"id\")\\\n",
    "    .first()\\\n",
    "    .rename(columns={'response': 'context'})\\\n",
    "    .reset_index()\n",
    "\n",
    "df_topical = df_topical[~df_topical.isin(context)]\n",
    "\n",
    "topical_preprocessed = df_topical\\\n",
    "    .set_index('id')\\\n",
    "    .join(context.set_index('id'))\\\n",
    "    .reset_index()[['context', 'response']]\n",
    "\n",
    "topical_preprocessed.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs count: 190741\n"
     ]
    }
   ],
   "source": [
    "contexts += list(topical_preprocessed.context)\n",
    "responses += list(topical_preprocessed.response)\n",
    "\n",
    "print(f\"Total pairs count: {len(contexts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cornell Movie Dialogue Dataset\n",
    "\n",
    "This corpus contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts: 220,579 conversational exchanges between 10,292 pairs of movie characters involving 9,035 characters from 617 movies.\n",
    "\n",
    "The preprocessing code is taken from https://www.kaggle.com/shashankasubrahmanya/preprocessing-cornell-movie-dialogue-corpus/\n",
    "Link to the dataset https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n",
    "\n",
    "### Create a list of dialogues\n",
    "\n",
    "We join two different files namely `movie_lines.tsv` and `movie_conversations.tsv` to finally produce a list of dialogues. This list is further stored as a `pickle` file for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LineID</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>L1045</td>\n",
       "      <td>They do not!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>L1044</td>\n",
       "      <td>They do to!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>L985</td>\n",
       "      <td>I hope so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>L984</td>\n",
       "      <td>She okay?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>L925</td>\n",
       "      <td>Let's go.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LineID           Line\n",
       "0  L1045   They do not!\n",
       "1  L1044    They do to!\n",
       "2   L985     I hope so.\n",
       "3   L984      She okay?\n",
       "4   L925      Let's go."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_lines_features = [\"LineID\", \"Character\", \"Movie\", \"Name\", \"Line\"]\n",
    "movie_lines = pd.read_csv(\n",
    "    \"Datasets/movie-dialogue/movie_lines.txt\",\n",
    "    sep = \"\\+\\+\\+\\$\\+\\+\\+\", \n",
    "    engine = \"python\", \n",
    "    index_col = False, \n",
    "    names = movie_lines_features,\n",
    ")\n",
    "\n",
    "# Using only the required columns, namely, \"LineID\" and \"Line\"\n",
    "movie_lines = movie_lines[[\"LineID\", \"Line\"]]\n",
    "\n",
    "# Strip the space from \"LineID\" for further usage and change the datatype of \"Line\"\n",
    "movie_lines[\"LineID\"] = movie_lines[\"LineID\"].apply(str.strip)\n",
    "\n",
    "movie_lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ['L194', 'L195', 'L196', 'L197']\n",
       "1                     ['L198', 'L199']\n",
       "2     ['L200', 'L201', 'L202', 'L203']\n",
       "3             ['L204', 'L205', 'L206']\n",
       "4                     ['L207', 'L208']\n",
       "Name: Conversation, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_conversations_features = [\"Character1\", \"Character2\", \"Movie\", \"Conversation\"]\n",
    "movie_conversations = pd.read_csv(\n",
    "    \"Datasets/movie-dialogue/movie_conversations.txt\",\n",
    "    sep = \"\\+\\+\\+\\$\\+\\+\\+\", \n",
    "    engine = \"python\", \n",
    "    index_col = False, \n",
    "    names = movie_conversations_features\n",
    ")\n",
    "\n",
    "# Again using the required feature, \"Conversation\"\n",
    "movie_conversations = movie_conversations[\"Conversation\"]\n",
    "\n",
    "movie_conversations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This instruction takes lot of time, run it only once.\n",
    "#conversation = [[str(list(movie_lines.loc[movie_lines[\"LineID\"] == u.strip().strip(\"'\"), \"Line\"])[0]).strip() for u in c.strip().strip('[').strip(']').split(',')] for c in movie_conversations]\n",
    "\n",
    "#with open(\"./conversations.pkl\", \"wb\") as handle:\n",
    " #   pkl.dump(conversation, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create context and response pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"You're asking me out.  That's so cute. What's your name again?\",\n",
       "  'Forget it.'],\n",
       " ['Gosh, if only we could find Kat a boyfriend...',\n",
       "  'Let me see what I can do.'],\n",
       " ['How is our little Find the Wench A Date plan progressing?',\n",
       "  \"Well, there's someone I think might be --\"],\n",
       " ['There.', 'Where?'],\n",
       " ['You got something on your mind?',\n",
       "  \"I counted on you to help my cause. You and that thug are obviously failing. Aren't we ever going on our date?\"],\n",
       " ['You have my word.  As a gentleman', \"You're sweet.\"],\n",
       " ['How do you get your hair to look like that?',\n",
       "  \"Eber's Deep Conditioner every two days. And I never, ever use a blowdryer without the diffuser attachment.\"],\n",
       " ['Hi.', 'Looks like things worked out tonight, huh?'],\n",
       " ['You know Chastity?', 'I believe we share an art instructor'],\n",
       " ['Have fun tonight?', 'Tons']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "with open(\"./conversations.pkl\", \"rb\") as handle:\n",
    "    conversation = pkl.load(handle)\n",
    "    conversation = list(filter(lambda dialogue: len(dialogue) == 2, conversation))\n",
    "\n",
    "conversation[:10]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"You're asking me out.  That's so cute. What's your name again?\",\n",
       "  'Forget it.'),\n",
       " ('Gosh, if only we could find Kat a boyfriend...',\n",
       "  'Let me see what I can do.'),\n",
       " ('How is our little Find the Wench A Date plan progressing?',\n",
       "  \"Well, there's someone I think might be --\"),\n",
       " ('There.', 'Where?'),\n",
       " ('You got something on your mind?',\n",
       "  \"I counted on you to help my cause. You and that thug are obviously failing. Aren't we ever going on our date?\"),\n",
       " ('You have my word.  As a gentleman', \"You're sweet.\"),\n",
       " ('How do you get your hair to look like that?',\n",
       "  \"Eber's Deep Conditioner every two days. And I never, ever use a blowdryer without the diffuser attachment.\"),\n",
       " ('Hi.', 'Looks like things worked out tonight, huh?'),\n",
       " ('You know Chastity?', 'I believe we share an art instructor'),\n",
       " ('Have fun tonight?', 'Tons')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_pairs(dialogues):\n",
    "    \n",
    "    context_list = []\n",
    "    response_list = []\n",
    "    \n",
    "    for dialogue in dialogues:        \n",
    "        context_list.append(dialogue[0])\n",
    "        response_list.append(dialogue[1])\n",
    "        \n",
    "    return context_list, response_list\n",
    "\n",
    "context_list, response_list = generate_pairs(conversation)\n",
    "\n",
    "list(zip(context_list, response_list))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Merge datasets\n",
    "contexts += context_list\n",
    "responses += response_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "Firstly we remove from the dataset all the pairs that don't contain any letters or the ones that were not written in English. Furthermore we exclude pairs, in which the length of either the context or the response exceeds 20 words/tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def filter(contexts,responses,threshold):\n",
    "    new_contexts = []\n",
    "    new_responses = []\n",
    "    for i in range(len(contexts)):\n",
    "        if isinstance(contexts[i],str) and isinstance(contexts[i],str):\n",
    "            if re.search('[a-zA-Z]',contexts[i]) != None and re.search('[a-zA-Z]',responses[i]) != None :\n",
    "                if len(contexts[i].split()) <= threshold and len(responses[i].split()) <= threshold and \\\n",
    "                detect(responses[i])=='en' and detect(contexts[i])=='en':\n",
    "                    new_contexts.append(contexts[i])\n",
    "                    new_responses.append(responses[i])\n",
    "    return new_contexts, new_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_timesteps = response_timesteps = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs count: 97119\n",
      "Total pairs count: 97119\n"
     ]
    }
   ],
   "source": [
    "\n",
    "contexts, responses = filter(contexts,responses,context_timesteps)\n",
    "\n",
    "print(f\"Total pairs count: {len(contexts)}\")\n",
    "print(f\"Total pairs count: {len(responses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language detection takes a long time, and therefore remember to save the results and load them from file in further executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs count: 97119\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total pairs count: {len(contexts)}\")\n",
    "contexts = np.array(contexts,dtype=str)\n",
    "responses = np.array(responses,dtype=str)\n",
    "\n",
    "\n",
    "pd.DataFrame(contexts).to_csv(\"contexts_20.csv\")\n",
    "pd.DataFrame(responses).to_csv(\"responses_20.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "We have 97119 record pairs in our dataset, but due to the restricted resources we are going to use only 48000 pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_timesteps = response_timesteps = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = np.squeeze(pd.read_csv(\"contexts_20.csv\", usecols=[1]).to_numpy()[:32000])\n",
    "responses = np.squeeze(pd.read_csv(\"responses_20.csv\",usecols=[1]).to_numpy()[:32000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split dataset into training and test subsets\n",
    "def shuffle_split_data(contexts,responses,train_size, random_seed=50):\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    #Shuffle indices\n",
    "    indices = np.arange(len(contexts))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    #Select indices for both train and test subsets\n",
    "    train_indices = indices[:train_size]\n",
    "    test_indices = indices[train_size:]\n",
    "    \n",
    "    #Split contexts and responses into train and test subsets\n",
    "    contexts_train = np.array([contexts[i] for i in train_indices],dtype=str)\n",
    "    contexts_test = np.array([contexts[i] for i in test_indices],dtype=str)\n",
    "    \n",
    "    responses_train = np.array([responses[i] for i in train_indices],dtype=str)\n",
    "    responses_test = np.array([responses[i] for i in test_indices],dtype=str)\n",
    "                              \n",
    "    return contexts_train,contexts_test,responses_train,responses_test\n",
    "\n",
    "# Mutate text to sequence of tokens\n",
    "def to_seq(tokenizer, text, pad_length=None, padding_type='post'):\n",
    "    encoded_text = tokenizer.texts_to_sequences(text)\n",
    "    preproc_text = pad_sequences(encoded_text, padding=padding_type, maxlen=pad_length)\n",
    "    \n",
    "    return preproc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_train, contexts_test, responses_train, responses_text = shuffle_split_data(contexts,responses,int(len(contexts)*4/5))\n",
    "\n",
    "context_tokenizer = keras.preprocessing.text.Tokenizer(oov_token='UNK')\n",
    "context_tokenizer.fit_on_texts(contexts_train)\n",
    "\n",
    "response_tokenizer = keras.preprocessing.text.Tokenizer(oov_token='UNK')\n",
    "response_tokenizer.fit_on_texts(responses_train)\n",
    "\n",
    "contexts_seq = context_tokenizer.texts_to_sequences(contexts_train)\n",
    "responses_seq = response_tokenizer.texts_to_sequences(contexts_train)\n",
    "\n",
    "contexts_seq = pad_sequences(contexts_seq,padding='post',maxlen=context_timesteps)\n",
    "responses_seq = pad_sequences(responses_seq,padding='post',maxlen=response_timesteps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "hidden_size = 96\n",
    "\n",
    "context_vsize = max(context_tokenizer.index_word.keys()) + 1\n",
    "response_vsize = max(response_tokenizer.index_word.keys()) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "\n",
    "The model that we are going to use is a seq2seq model created by Thushan Ganegedara (https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39). The author intented to implement this model for machine translation purposes, however we are going to use it for text generation of our chatbot. The model consists of an Encoder and Decoder build on GRU's, and an additional Attention Layer definded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
    "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state\n",
    "            inputs: (batchsize * 1 * de_in_dim)\n",
    "            states: (batchsize * 1 * de_latent_dim)\n",
    "            \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch size * en_seq_len * latent_dim\n",
    "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print('Ua.h>', U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
    "            if verbose:\n",
    "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print('ei>', e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
    "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Input, GRU, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "def define_nmt(hidden_size, batch_size, con_timesteps, con_vsize, res_timesteps, res_vsize):\n",
    "    \"\"\" Defining a NMT model \"\"\"\n",
    "\n",
    "    # Define an input sequence and process it.\n",
    "    if batch_size:\n",
    "        encoder_inputs = Input(batch_shape=(batch_size, con_timesteps, con_vsize), name='encoder_inputs')\n",
    "        decoder_inputs = Input(batch_shape=(batch_size, res_timesteps - 1, res_vsize), name='decoder_inputs')\n",
    "    else:\n",
    "        encoder_inputs = Input(shape=(con_timesteps, con_vsize), name='encoder_inputs')\n",
    "        if res_timesteps:\n",
    "            decoder_inputs = Input(shape=(res_timesteps - 1, res_vsize), name='decoder_inputs')\n",
    "        else:\n",
    "            decoder_inputs = Input(shape=(None, res_vsize), name='decoder_inputs')\n",
    "\n",
    "    # Encoder GRU\n",
    "    encoder_gru = GRU(hidden_size, return_sequences=True, return_state=True, name='encoder_gru')\n",
    "    encoder_out, encoder_state = encoder_gru(encoder_inputs)\n",
    "\n",
    "    # Set up the decoder GRU, using `encoder_states` as initial state.\n",
    "    decoder_gru = GRU(hidden_size, return_sequences=True, return_state=True, name='decoder_gru')\n",
    "    decoder_out, decoder_state = decoder_gru(decoder_inputs, initial_state=encoder_state)\n",
    "\n",
    "    # Attention layer\n",
    "    attn_layer = AttentionLayer(name='attention_layer')\n",
    "    attn_out, attn_states = attn_layer([encoder_out, decoder_out])\n",
    "\n",
    "    # Concat attention input and decoder GRU output\n",
    "    decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_out, attn_out])\n",
    "\n",
    "    # Dense layer\n",
    "    dense = Dense(res_vsize, activation='softmax', name='softmax_layer')\n",
    "    dense_time = TimeDistributed(dense, name='time_distributed_layer')\n",
    "    decoder_pred = dense_time(decoder_concat_input)\n",
    "\n",
    "    # Full model\n",
    "    full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
    "    full_model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'], sample_weight_mode='temporal')\n",
    "\n",
    "    full_model.summary()\n",
    "    \n",
    "    \"\"\" Inference model \"\"\"\n",
    "    batch_size = 1\n",
    "\n",
    "    \"\"\" Encoder (Inference) model \"\"\"\n",
    "    encoder_inf_inputs = Input(batch_shape=(batch_size, con_timesteps, con_vsize), name='encoder_inf_inputs')\n",
    "    encoder_inf_out, encoder_inf_state = encoder_gru(encoder_inf_inputs)\n",
    "    encoder_model = Model(inputs=encoder_inf_inputs, outputs=[encoder_inf_out, encoder_inf_state])\n",
    "\n",
    "    \"\"\" Decoder (Inference) model \"\"\"\n",
    "    decoder_inf_inputs = Input(batch_shape=(batch_size, 1, res_vsize), name='decoder_word_inputs')\n",
    "    encoder_inf_states = Input(batch_shape=(batch_size, con_timesteps, hidden_size), name='encoder_inf_states')\n",
    "    decoder_init_state = Input(batch_shape=(batch_size, hidden_size), name='decoder_init')\n",
    "\n",
    "    decoder_inf_out, decoder_inf_state = decoder_gru(decoder_inf_inputs, initial_state=decoder_init_state)\n",
    "    attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
    "    decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_inf_out, attn_inf_out])\n",
    "    decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
    "    decoder_model = Model(inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n",
    "                          outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state])\n",
    "    \n",
    "    return full_model,encoder_model,decoder_model\n",
    "\n",
    "def infer_nmt(encoder_model, decoder_model, test_en_seq, en_vsize, fr_vsize):\n",
    "    \"\"\"\n",
    "    Infer logic\n",
    "    :param encoder_model: keras.Model\n",
    "    :param decoder_model: keras.Model\n",
    "    :param test_en_seq: sequence of word ids\n",
    "    :param en_vsize: int\n",
    "    :param fr_vsize: int\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    test_fr_seq = sents2sequences(response_tokenizer, ['sos'], fr_vsize)\n",
    "    test_en_onehot_seq = to_categorical(test_en_seq, num_classes=en_vsize)\n",
    "    test_fr_onehot_seq = np.expand_dims(to_categorical(test_fr_seq, num_classes=fr_vsize), 1)\n",
    "\n",
    "    enc_outs, enc_last_state = encoder_model.predict(test_en_onehot_seq)\n",
    "    dec_state = enc_last_state\n",
    "    attention_weights = []\n",
    "    fr_text = ''\n",
    "    for i in range(20):\n",
    "\n",
    "        dec_out, attention, dec_state = decoder_model.predict([enc_outs, dec_state, test_fr_onehot_seq])\n",
    "        dec_ind = np.argmax(dec_out, axis=-1)[0, 0]\n",
    "        \n",
    "\n",
    "        if dec_ind == 0:\n",
    "            break\n",
    "        test_fr_seq = sents2sequences(response_tokenizer, [response_index2word[dec_ind]], fr_vsize)\n",
    "        test_fr_onehot_seq = np.expand_dims(to_categorical(test_fr_seq, num_classes=fr_vsize), 1)\n",
    "\n",
    "        attention_weights.append((dec_ind, attention))\n",
    "        fr_text += response_index2word[dec_ind] + ' '\n",
    "\n",
    "    return fr_text, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(64, 20, 3568)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(64, 19, 13543)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_gru (GRU)               [(64, 20, 96), (64,  1055520     encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru (GRU)               [(64, 19, 96), (64,  3928320     decoder_inputs[0][0]             \n",
      "                                                                 encoder_gru[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((64, 19, 96), (64,  18528       encoder_gru[0][0]                \n",
      "                                                                 decoder_gru[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (64, 19, 192)        0           decoder_gru[0][0]                \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_layer (TimeDis (64, 19, 13543)      2613799     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 7,616,167\n",
      "Trainable params: 7,616,167\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model creation\n",
    "full_model, encoder_model, decoder_model = define_nmt(\n",
    "        hidden_size=hidden_size, batch_size=batch_size,\n",
    "        con_timesteps=context_timesteps, res_timesteps=response_timesteps,\n",
    "        con_vsize=context_vsize, res_vsize=response_vsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Instead of using the fit() method, we will rely on the train_on_batch method, that allows us to spare some memory, but in return it requires us to manually handle the epochs and batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 1: 0.5446655040041994\n",
      "Loss in epoch 2: 0.5367221447655506\n",
      "Loss in epoch 3: 0.5297323799596394\n",
      "Loss in epoch 4: 0.5254579814206949\n",
      "Loss in epoch 5: 0.5204291006977341\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    losses = []\n",
    "    for bi in range(0, contexts_seq.shape[0] - batch_size, batch_size):\n",
    "\n",
    "        context_onehot_seq = to_categorical(contexts_seq[bi:bi + batch_size, :], num_classes=context_vsize)\n",
    "        response_onehot_seq = to_categorical(responses_seq[bi:bi + batch_size, :], num_classes=response_vsize)\n",
    "\n",
    "        full_model.train_on_batch([context_onehot_seq, response_onehot_seq[:, :-1, :]], response_onehot_seq[:, 1:, :])\n",
    "\n",
    "        l = full_model.evaluate([context_onehot_seq, response_onehot_seq[:, :-1, :]], response_onehot_seq[:, 1:, :],\n",
    "                                batch_size=batch_size, verbose=0)\n",
    "\n",
    "        losses.append(l)\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(\"Loss in epoch {}: {}\".format(epoch + 1, np.mean(losses)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(full_model, 'model_v5.h5')\n",
    "tf.keras.models.save_model(encoder_model, 'encoder_v5.h5')\n",
    "tf.keras.models.save_model(decoder_model, 'decoder_v5.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of retraining the model we are going to load our already trained model (15 epochs) from file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# full_model =  tf.keras.models.load_model('model_v4.h5', custom_objects={'AttentionLayer':AttentionLayer })\n",
    "encoder_model =  tf.keras.models.load_model('encoder_v5.h5', custom_objects={'AttentionLayer':AttentionLayer })\n",
    "decoder_model =  tf.keras.models.load_model('decoder_v5.h5', custom_objects={'AttentionLayer':AttentionLayer })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(64, 20, 3568)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(64, 19, 13543)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_gru (GRU)               multiple             1055520     encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru (GRU)               multiple             3928320     decoder_inputs[0][0]             \n",
      "                                                                 encoder_gru[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer multiple             18528       encoder_gru[0][0]                \n",
      "                                                                 decoder_gru[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (64, 19, 192)        0           decoder_gru[0][0]                \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_layer (TimeDis (64, 19, 13543)      2613799     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 7,616,167\n",
      "Trainable params: 7,616,167\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating vocab dictionaries and inference models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Index2word \"\"\"\n",
    "context_index2word = dict(zip(context_tokenizer.word_index.values(), context_tokenizer.word_index.keys()))\n",
    "response_index2word = dict(zip(response_tokenizer.word_index.values(), response_tokenizer.word_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: hey\n",
      "\n",
      "Generated Response: \n",
      "\n",
      "Generated Response: hey there did you rest well\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def sents2sequences(tokenizer, sentences, reverse=False, pad_length=None, padding_type='post'):\n",
    "    encoded_text = tokenizer.texts_to_sequences(sentences)\n",
    "    preproc_text = pad_sequences(encoded_text, padding=padding_type, maxlen=pad_length)\n",
    "    if reverse:\n",
    "        preproc_text = np.flip(preproc_text, axis=1)\n",
    "\n",
    "    return preproc_text\n",
    "\n",
    "\"\"\" Inferring with trained model \"\"\"\n",
    "single_test = contexts[92]\n",
    "print('Context: {}'.format(single_test))\n",
    "\n",
    "single_test_seq = sents2sequences(context_tokenizer, [single_test], pad_length=context_timesteps)\n",
    "response, attn_weights = infer_nmt(\n",
    "    encoder_model=encoder_model, decoder_model=decoder_model,\n",
    "    test_en_seq=single_test_seq, en_vsize=context_vsize, fr_vsize=response_vsize)\n",
    "print('\\nGenerated Response: {}'.format(response))\n",
    "print('\\nGenerated Response: {}'.format(responses[92]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_response(test_input):\n",
    "    #Getting the output states to pass into the decoder\n",
    "    states_value = encoder_model.predict(test_input)\n",
    "    #Generating empty target sequence of length 1\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    #Setting the first token of target sequence with the start token\n",
    "    target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
    "    \n",
    "    #A variable to store our response word by word\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    stop_condition = False\n",
    "    while not stop_condition:\n",
    "          #Predicting output tokens with probabilities and states\n",
    "          output_tokens, hidden_state, cell_state = decoder_model.predict([target_seq] + states_value)\n",
    "            #Choosing the one with highest probability\n",
    "          sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "          sampled_token = reverse_target_features_dict[sampled_token_index]\n",
    "          decoded_sentence += \" \" + sampled_token\n",
    "            #Stop if hit max length or found the stop token\n",
    "          if (sampled_token == '<END>' or len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "          #Update the target sequence\n",
    "          target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "          target_seq[0, 0, sampled_token_index] = 1.\n",
    "          #Update states\n",
    "          states_value = [hidden_state, cell_state]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    negative_responses = (\"no\", \"nope\", \"nah\", \"naw\", \"not a chance\", \"sorry\")\n",
    "    exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\")\n",
    "    #Method to start the conversation\n",
    "    def start_chat(self):\n",
    "        user_response = input(\"Hi, I'm a chatbot trained on random dialogs. Would you like to chat with me?\\n\")\n",
    "\n",
    "        if user_response in self.negative_responses:\n",
    "          print(\"Ok, have a great day!\")\n",
    "          return\n",
    "        self.chat(user_response)\n",
    "    #Method to handle the conversation\n",
    "    def chat(self, reply):\n",
    "        while not self.make_exit(reply):\n",
    "            reply = input(self.generate_response(reply)+\"\\n\")\n",
    "\n",
    "    #Method that will create a response using seq2seq model we built\n",
    "    def generate_response(self, user_input):\n",
    "        input_seq = sents2sequences(context_tokenizer, [user_input], pad_length=context_timesteps)\n",
    "        chatbot_response, attn_weights = infer_nmt(\n",
    "            encoder_model=encoder_model_1, decoder_model=decoder_model_1,\n",
    "            test_en_seq=input_seq, en_vsize=context_vsize, fr_vsize=response_vsize)\n",
    "        return chatbot_response\n",
    "    #Method to check for exit commands\n",
    "    def make_exit(self, reply):\n",
    "        for exit_command in self.exit_commands:\n",
    "            if exit_command in reply:\n",
    "                print(\"Ok, have a great day!\")\n",
    "                return True\n",
    "        return False\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I'm a chatbot trained on random dialogs. Would you like to chat with me?\n",
      "Hello there\n",
      "there \n",
      "How is your day going\n",
      "how often do you wear trousers \n",
      "Quite often\n",
      "how about \n",
      "I dont understand you fully\n",
      "you do you wear them \n",
      "Do you like fishing\n",
      "do you wear starbucks \n",
      "Disco time!\n",
      "\n",
      "That was a surprise for you\n",
      "are very a fan of a marvel \n",
      "Dear chatbot, you are an idiot\n",
      "is very good thing \n",
      "Just as your creator.\n",
      "UNK UNK UNK UNK UNK \n",
      "That is not funny\n",
      "i was curious thing in ai \n",
      "Shut up\n",
      "\n",
      "Nobody taught you how to swear?\n",
      "you cant UNK UNK \n",
      "I count that as swearing\n",
      "UNK that me please \n",
      "quit\n",
      "Ok, have a great day!\n"
     ]
    }
   ],
   "source": [
    "chatbot = ChatBot()\n",
    "chatbot.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
